# GRPO与PPO算法对比分析

![GRPO与PPO算法对比图](../img/grpo_ppo.jpg)

这张图可能大家看过无数次了，但是究竟如何理解？今天我们用更易于理解的语言来解释一下，看看GRPO究竟厉害在哪，为什么deepseek要使用这个算法来训练R1模型。

## 1. GRPO和PPO原理解析

### PPO原理（图上半部分）

PPO（Proximal Policy Optimization，近端策略优化）是一种广泛应用的强化学习算法，其工作流程如图所示：

1. **输入处理**：输入查询q（如问题或任务描述）被送入策略模型。这相当于向系统提出一个需要解决的问题。
2. **单一输出生成**：策略模型（通常是语言模型llm）生成单一输出o。系统基于当前策略生成一个解决方案。
3. **多重评估**：系统从多个角度评估这个解决方案
   - 参考模型计算KL散度（Kullback-Leibler divergence），确保新策略不会偏离太远。这可以理解为确保系统不会突然改变其行为方式。
   - 奖励模型评估输出质量，给出奖励r。这相当于对解决方案的质量进行评分。
   - 价值模型预测未来奖励的期望值v。这相当于预测该解决方案长期来看有多大价值。
4. **优势计算**：使用广义优势估计（GAE, Generalized Advantage Estimation）结合即时奖励r和价值估计v计算优势A。这一步计算当前解决方案比预期表现好多少。
5. **策略更新**：基于优势A和KL约束更新策略模型。系统根据评估结果调整其策略，但不会做出过大改变。

PPO的核心是通过"信任区域"方法限制策略更新步长，使用裁剪目标函数：
```
L = min(ratio * advantage, clip(ratio, 1-ε, 1+ε) * advantage)
```
其中ratio是新旧策略的概率比，表示策略变化的程度。这种方法确保策略更新不会过大，保持学习的稳定性。

### GRPO原理（图下半部分）

GRPO（Group Relative Policy Optimization，组相对策略优化）是PPO的一个变体，专为语言模型微调设计：

1. **输入处理**：与PPO相同，输入查询q被送入策略模型。
2. **多样性输出生成**：策略模型为同一输入生成多个不同输出(o₁, o₂, ..., oG)。这是一个关键区别，系统不再只生成一个解决方案，而是同时生成多个不同的解决方案。
3. **多重评估**：
   - 参考模型计算KL散度，同样确保策略不会偏离太远。
   - 奖励模型为每个输出评分，得到多个奖励值(r₁, r₂, ..., rG)。每个解决方案都获得独立的评分。
4. **组内优势计算**：通过"Group Computation"模块，基于同组内的奖励计算相对优势(A₁, A₂, ..., AG)。这一步直接比较同一组内不同解决方案的相对优劣，而不依赖于价值预测。
5. **策略更新**：基于相对优势和KL约束更新策略模型。系统学习产生更好的解决方案，避免产生较差的解决方案。

GRPO的关键创新是去除了价值模型，转而使用组内统计计算优势。这种方法不再需要预测未来奖励，而是直接比较当前生成的多个解决方案的相对质量。

## 2. GRPO相比PPO的创新点

从图中可以清晰看出GRPO的三大创新：

### 1. 多样性采样

PPO对每个输入只生成一个输出o，而GRPO生成多个输出(o₁, o₂, ..., oG)。这种多样性采样有几个明显优势：
- 更全面地探索解空间，相当于同时考虑多种可能的解决方案
- 提供更丰富的学习信号，可以从多个解决方案中获取更多信息
- 更适合处理有多种正确答案的语言任务，特别是开放性问题

### 2. 组内相对优势计算

PPO使用价值模型预测的价值v与实际奖励r的差值计算优势。GRPO则通过组内统计直接计算相对优势：
- 计算同组内奖励的均值和标准差，相当于建立一个内部基准
- 使用(reward - group_mean) / group_std计算相对优势，评估每个解决方案相对于组内平均水平的优劣
- 这种归一化处理使算法对不同难度问题的奖励尺度差异不敏感，无论问题难易程度如何，都能进行公平比较

从图中可以看到，GRPO用"Group Computation"（组计算）模块替代了PPO中的"GAE"模块，实现了无需价值模型的优势计算。

### 3. 去除价值网络

最显著的创新是完全去除了价值网络（图中黄色的"Value Model"）：
- PPO需要训练一个单独的价值网络预测未来奖励，这需要额外的计算资源
- GRPO直接使用组内统计作为基线，不需要额外的价值网络
- 这大大简化了算法架构，同时保持或提高了性能

## 3. GRPO为什么消耗更少的资源

图中右侧标注了"Trained Models"（黄色，需要训练的模型）和"Frozen Models"（蓝色，固定不变的模型）。GRPO的核心优势在于去除了价值网络，带来以下好处：

| 优势类别 | GRPO相比PPO的具体优势 |
|---------|----------------------|
| **参数量减少** | 无需训练价值网络，训练参数量减少约50% |
| **计算资源节省** | 无需价值网络的前向和后向传播，计算量显著降低 |
| **超参数简化** | 无需调整价值网络相关超参数，如价值损失系数、GAE参数λ等 |
| **训练稳定性** | 避免价值估计误差带来的不稳定性，减少训练失败风险 |

总的来说，GRPO通过去除价值网络，在保持或提高性能的同时，显著降低了计算资源需求和实现复杂度，使大规模语言模型的强化学习训练更加高效和稳定。

## 总结

GRPO算法通过三个关键创新（多样性采样、组内相对优势计算和去除价值网络）在保持PPO核心思想的同时，显著降低了资源消耗。
